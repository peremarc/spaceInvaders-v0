{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21f81d4",
   "metadata": {},
   "source": [
    "# SpaceInvaders DQN (self-contained)\n",
    "\n",
    "Todo el código está en este notebook: wrappers, modelo, agente, entrenamiento/evaluación y análisis del log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb377b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install #   \"tensorflow==2.10.0\" #   \"keras-rl2==1.0.5\" #   \"gym==0.25.2\" #   \"ale-py==0.7.5\" #   \"opencv-python<4.8\" #   \"matplotlib<3.6\" #   \"numpy<1.24\" #   \"Pillow<10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bdb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from rl.core import Processor\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Permute\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7b5d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración básica y rutas\n",
    "BASE_DIR = Path.cwd()\n",
    "WEIGHTS_DIR = BASE_DIR / \"weights\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENV_NAME = \"SpaceInvaders-v0\"\n",
    "SEED = 42\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "STAGE_STEPS_DEFAULT = 3_000_000\n",
    "WEIGHTS_PATH = WEIGHTS_DIR / \"dqn_spaceinvaders_v0_weights.h5f\"\n",
    "LOG_PATH = LOGS_DIR / \"training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191f3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers compatibles con gym clásico y keras-rl2\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"Frame-skip + max-pooling de los últimos 2 frames.\"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        info = {}\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            terminated = terminated or done\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.maximum(self._obs_buffer[0], self._obs_buffer[-1])\n",
    "        done = terminated\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"Asegura que el juego arranque (SpaceInvaders puede requerir FIRE).\"\"\"\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset()\n",
    "        obs, _, done, info = self.env.step(1)\n",
    "        if done:\n",
    "            obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"Ejecuta un número aleatorio de acciones NOOP tras reset.\"\"\"\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        n_noops = np.random.randint(1, self.noop_max + 1)\n",
    "        for _ in range(n_noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    \"\"\"Marca done=True al perder una vida (no es game over), solo para train.\"\"\"\n",
    "    def __init__(self, env, lives_key=\"ale.lives\"):\n",
    "        super().__init__(env)\n",
    "        self.lives_key = lives_key\n",
    "        self.lives = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self.lives = 0\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        lives = info.get(self.lives_key, None)\n",
    "        if lives is not None:\n",
    "            if self.lives == 0:\n",
    "                self.lives = lives\n",
    "            if (lives < self.lives) and (lives > 0):\n",
    "                done = True\n",
    "            self.lives = lives\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1f1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor para preprocesado Atari\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, INPUT_SHAPE, interpolation=cv2.INTER_AREA)\n",
    "        return resized.astype(np.uint8)\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch.astype(\"float32\") / 255.0\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86482beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN\n",
    "def build_model(nb_actions: int):\n",
    "    model = Sequential()\n",
    "    model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation=\"relu\"))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation=\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dense(nb_actions, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d81f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entorno y semillas\n",
    "def make_env(seed: int, training: bool):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = FireResetEnv(env)\n",
    "    if training:\n",
    "        env = EpisodicLifeEnv(env, lives_key=\"ale.lives\")\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    try:\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "    except TypeError:\n",
    "        env.seed(seed)\n",
    "        env.reset()\n",
    "    return env\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee94640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente DQN\n",
    "def build_agent(env):\n",
    "    nb_actions = env.action_space.n\n",
    "    model = build_model(nb_actions)\n",
    "\n",
    "    memory = SequentialMemory(limit=1_000_000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=0.02, # 0.1, 1.0\n",
    "        value_min=0.005, # 0.02, 0.1\n",
    "        value_test=0,\n",
    "        nb_steps=300_000, # 500_000, 1_000_000\n",
    "    )\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=nb_actions,\n",
    "        policy=policy,\n",
    "        memory=memory,\n",
    "        processor=AtariProcessor(),\n",
    "        nb_steps_warmup=20_000, # 100_000\n",
    "        gamma=0.99,\n",
    "        target_model_update=5e-3,\n",
    "        batch_size=32,\n",
    "        train_interval=4,\n",
    "        enable_double_dqn=True,\n",
    "        enable_dueling_network=True,\n",
    "        dueling_type=\"avg\",\n",
    "        delta_clip=1.0,\n",
    "    )\n",
    "\n",
    "    dqn.compile(Adam(learning_rate=5e-5, clipnorm=10.0), metrics=[\"mae\"]) # 1e-4\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0968070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y evaluación\n",
    "def train(dqn, env, nb_steps: int = STAGE_STEPS_DEFAULT):\n",
    "    WEIGHTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    weights_index = Path(f\"{WEIGHTS_PATH}.index\")\n",
    "    if weights_index.exists():\n",
    "        print(f\"Cargando pesos: {WEIGHTS_PATH}\")\n",
    "        dqn.load_weights(str(WEIGHTS_PATH))\n",
    "    else:\n",
    "        print(\"No se encontraron pesos previos, entrenando desde cero.\")\n",
    "\n",
    "    dqn.fit(\n",
    "        env,\n",
    "        nb_steps=nb_steps,\n",
    "        log_interval=10_000,\n",
    "        visualize=False,\n",
    "        verbose=2,\n",
    "        callbacks=[\n",
    "            FileLogger(str(LOG_PATH), interval=100),\n",
    "            ModelIntervalCheckpoint(str(WEIGHTS_PATH), interval=50_000),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    dqn.save_weights(str(WEIGHTS_PATH), overwrite=True)\n",
    "    print(f\"Pesos guardados en: {WEIGHTS_PATH}\")\n",
    "\n",
    "\n",
    "def evaluate(dqn, env, episodes: int = 110):\n",
    "    print(f\"Iniciando evaluación ({episodes} episodios)...\")\n",
    "    history = dqn.test(env, nb_episodes=episodes, visualize=False, verbose=0)\n",
    "    rewards = history.history[\"episode_reward\"]\n",
    "    last_100 = rewards[-100:] if len(rewards) >= 100 else rewards\n",
    "    min_score = float(np.min(last_100))\n",
    "    mean_score = float(np.mean(last_100))\n",
    "    print(f\"Min reward (últimos {len(last_100)}): {min_score}\")\n",
    "    print(f\"Mean reward (últimos {len(last_100)}): {mean_score}\")\n",
    "    if len(last_100) >= 100 and min_score > 20:\n",
    "        print(\"ESTADO: REQUISITO CUMPLIDO (min(last100) > 20)\")\n",
    "    else:\n",
    "        print(\"ESTADO: REQUISITO NO CUMPLIDO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d4e33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades de análisis\n",
    "import json\n",
    "\n",
    "def load_training_log(path: Path = LOG_PATH):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def reward_summary(log_data):\n",
    "    rewards = log_data.get(\"episode_reward\")\n",
    "    if not rewards:\n",
    "        return {}\n",
    "    rewards_arr = np.asarray(rewards, dtype=float)\n",
    "    return {\n",
    "        \"count\": int(rewards_arr.size),\n",
    "        \"mean\": float(np.mean(rewards_arr)),\n",
    "        \"std\": float(np.std(rewards_arr)),\n",
    "        \"min\": float(np.min(rewards_arr)),\n",
    "        \"max\": float(np.max(rewards_arr)),\n",
    "        \"last_mean_100\": float(np.mean(rewards_arr[-100:])) if rewards_arr.size >= 1 else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "def plot_metric(log_data, metric: str, save_path=None):\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError as exc:\n",
    "        raise ImportError(\"matplotlib no está disponible en el entorno actual.\") from exc\n",
    "    if metric not in log_data:\n",
    "        raise KeyError(f\"Métrica '{metric}' no encontrada en el log.\")\n",
    "    values = log_data[metric]\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(values)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bbe9f",
   "metadata": {},
   "source": [
    "## Flujo rápido\n",
    "1. Fijar semillas y crear envs.\n",
    "2. Construir agente.\n",
    "3. Entrenar (opcional) y evaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82adc767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 12:42:00.139641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/runner/workspace/.venv-rl2/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2025-12-29 12:42:00.139666: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-12-29 12:42:00.139679: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0b3794c84d26): /proc/driver/nvidia/version does not exist\n",
      "2025-12-29 12:42:00.140293: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-29 12:42:00.150503: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2699995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dueling activado: True\n"
     ]
    }
   ],
   "source": [
    "set_seeds(SEED)\n",
    "env_train = make_env(SEED, training=True)\n",
    "env_test = make_env(SEED, training=False)\n",
    "dqn = build_agent(env_train)\n",
    "print('Dueling activado:', dqn.enable_dueling_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de7e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron pesos previos, entrenando desde cero.\n",
      "Training for 3000000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/workspace/.venv-rl2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      55/3000000: episode: 1, duration: 0.345s, episode steps:  55, steps per second: 159, episode reward:  6.000, mean reward:  0.109 [ 0.000,  1.000], mean action: 3.982 [3.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     117/3000000: episode: 2, duration: 0.336s, episode steps:  62, steps per second: 184, episode reward:  8.000, mean reward:  0.129 [ 0.000,  1.000], mean action: 3.903 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     170/3000000: episode: 3, duration: 0.234s, episode steps:  53, steps per second: 226, episode reward:  8.000, mean reward:  0.151 [ 0.000,  1.000], mean action: 3.925 [2.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     216/3000000: episode: 4, duration: 0.241s, episode steps:  46, steps per second: 191, episode reward:  4.000, mean reward:  0.087 [ 0.000,  1.000], mean action: 4.043 [4.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     277/3000000: episode: 5, duration: 0.292s, episode steps:  61, steps per second: 209, episode reward:  8.000, mean reward:  0.131 [ 0.000,  1.000], mean action: 3.984 [3.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     333/3000000: episode: 6, duration: 0.307s, episode steps:  56, steps per second: 183, episode reward:  8.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 3.946 [1.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     394/3000000: episode: 7, duration: 0.255s, episode steps:  61, steps per second: 239, episode reward:  8.000, mean reward:  0.131 [ 0.000,  1.000], mean action: 3.967 [2.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     449/3000000: episode: 8, duration: 0.281s, episode steps:  55, steps per second: 196, episode reward:  8.000, mean reward:  0.145 [ 0.000,  1.000], mean action: 3.982 [3.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     505/3000000: episode: 9, duration: 0.294s, episode steps:  56, steps per second: 191, episode reward:  8.000, mean reward:  0.143 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     565/3000000: episode: 10, duration: 0.302s, episode steps:  60, steps per second: 199, episode reward:  8.000, mean reward:  0.133 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     623/3000000: episode: 11, duration: 0.267s, episode steps:  58, steps per second: 217, episode reward:  8.000, mean reward:  0.138 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     683/3000000: episode: 12, duration: 0.318s, episode steps:  60, steps per second: 189, episode reward:  8.000, mean reward:  0.133 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     744/3000000: episode: 13, duration: 0.363s, episode steps:  61, steps per second: 168, episode reward:  8.000, mean reward:  0.131 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     804/3000000: episode: 14, duration: 0.276s, episode steps:  60, steps per second: 218, episode reward:  8.000, mean reward:  0.133 [ 0.000,  1.000], mean action: 3.983 [3.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     862/3000000: episode: 15, duration: 0.331s, episode steps:  58, steps per second: 175, episode reward:  8.000, mean reward:  0.138 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     922/3000000: episode: 16, duration: 0.506s, episode steps:  60, steps per second: 119, episode reward:  8.000, mean reward:  0.133 [ 0.000,  1.000], mean action: 3.883 [0.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     977/3000000: episode: 17, duration: 0.275s, episode steps:  55, steps per second: 200, episode reward:  8.000, mean reward:  0.145 [ 0.000,  1.000], mean action: 4.000 [4.000, 4.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "done, took 5.248 seconds\n",
      "Pesos guardados en: /home/runner/workspace/notebooks/project/notebooks/weights/dqn_spaceinvaders_v0_weights.h5f\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento. Ajustar nb_steps para pruebas cortas.\n",
    "train(dqn, env_train, nb_steps=STAGE_STEPS_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e9d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando pesos: /home/runner/workspace/notebooks/project/notebooks/weights/dqn_spaceinvaders_v0_weights.h5f\n",
      "Iniciando evaluación (110 episodios)...\n",
      "Min reward (últimos 100): 28.0\n",
      "Mean reward (últimos 100): 38.78\n",
      "ESTADO: REQUISITO CUMPLIDO (min(last100) > 20)\n"
     ]
    }
   ],
   "source": [
    "# Evaluación: carga pesos si existen y evalúa.\n",
    "if Path(f\"{WEIGHTS_PATH}.index\").exists():\n",
    "    print('Cargando pesos:', WEIGHTS_PATH)\n",
    "    dqn.load_weights(str(WEIGHTS_PATH))\n",
    "    evaluate(dqn, env_test, episodes=110)\n",
    "else:\n",
    "    print('No se encontraron pesos en', WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76fa0f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 17, 'mean': 7.647058823529412, 'std': 1.0256232808330996, 'min': 4.0, 'max': 8.0, 'last_mean_100': 7.647058823529412}\n"
     ]
    }
   ],
   "source": [
    "# Análisis del log\n",
    "try:\n",
    "    log_data = load_training_log(LOG_PATH)\n",
    "    print(reward_summary(log_data))\n",
    "    # fig, ax = plot_metric(log_data, 'loss')\n",
    "    # fig\n",
    "except FileNotFoundError:\n",
    "    print('No se encontró training_log.json en', LOG_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
