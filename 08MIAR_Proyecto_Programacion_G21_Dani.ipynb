{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDEQHM5MT1DP",
    "tags": []
   },
   "source": [
    "# 08MIAR - Aprendizaje Por Refuerzo: Proyecto de programación \"*Entrenamiento Agente Para Space Invaders*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CGiwwjYThj3"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##### Autores:    Daniel Romero Martinez / Roberto Vazquez Calvo / Pere Marc Monserrat Calbo\n",
    "##### Fecha:      Enero 2026.\n",
    "##### Titulación: Master Universitario en Inteligencia Artificial - VIU\n",
    "##### Profesor:   D. Jose Manuel Camacho\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "# 0. Enunciado del Problema\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga **alcanzar más de 20 puntos con reward clipping durante más de 100 episodios consecutivos**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Instalación y preparacion del entorno\n",
    "Todo el codigo que prepara el entorno en base a si estamos en Colab o en Local se ha llevado a una funcion ejecutable para mantener el notebook mas limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estamos en entorno Local: /home/rober/anaconda3/envs/GymClasic/bin/python\n",
      "Directorio actual: /home/rober/proyectos_python/08MIAR_AprendizajePorRefuerzo/Portfolio/Proyecto_Final\n",
      "Archivos en el directorio:\n",
      "['Solucion1', 'entorno_spaceinvadersv0.py', 'Solucion3', 'Anexo_MEMORIA.md', '__pycache__', 'img', 'env_functions.py', '08MIAR_Proyecto_Programacion_G21_Final.ipynb', '.ipynb_checkpoints']\n",
      "\n",
      "La version de Python instalada en este entorno Local es: 3.8\n",
      "\n",
      "Comprobando Paquetes Instalados....\n",
      "\n",
      "✔ numpy ya está instalado.\n",
      "✔ gym ya está instalado.\n",
      "✔ atari_py ya está instalado.\n",
      "✔ pyglet ya está instalado.\n",
      "✔ h5py ya está instalado.\n",
      "✔ Pillow ya está instalado.\n",
      "✔ keras-rl2 ya está instalado.\n",
      "✔ Keras ya está instalado.\n",
      "✔ tensorflow ya está instalado.\n",
      "✔ opencv-python-headless ya está instalado.\n",
      "✔ matplotlib ya está instalado.\n",
      "\n",
      "El entorno esta listo.\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a funcion que prepara el entorno.\n",
    "!python entorno_spaceinvadersv0.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "# 2. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Declaraciones Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "# Llamada a funciones generales\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, Layer, Permute\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, Memory\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Llamada a funciones propias\n",
    "from env_functions import plot_rewards, agent_eval, ShowLastTraining, merge_json_logs\n",
    "from env_functions import set_seeds, make_env, AtariProcessor, TrainAgent, TestAgent\n",
    "import env_functions as ef\n",
    "\n",
    "# Anulamos UserWarnings de TensorFlow\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tensorflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Solución nº2 - Double DQN\n",
    "En esta solucion implementamos una arquitectura doble DQN...\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/Double_DQN_img.png\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "La red Double DQN ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### 2.3.1 Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones del juego: 6\n",
      "Tipo de acciones disponibles en el juego: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "Dimensiones del Frame del juego: [210,160,3]\n"
     ]
    }
   ],
   "source": [
    "# Definicion de parametros fijos y carga de los entornos de training & test.\n",
    "# Parametros fijos\n",
    "seed = 42\n",
    "Memory_Size = 500000\n",
    "N_Solucion = 2\n",
    "Reward_min = 20\n",
    "Episodios_min = 100\n",
    "Training_Steps = 1500000\n",
    "\n",
    "# Definimos la semilla para la reproducibilidad de la ejecución\n",
    "set_seeds(seed)\n",
    "\n",
    "# Cargamos entornos de simulacion\n",
    "env_tr = make_env(seed, training=True)\n",
    "env_te = make_env(seed, training=False)\n",
    "\n",
    "# Extraemos informacion sobre las acciones y el espacio de observacion del entorno\n",
    "nb_actions = env_tr.action_space.n\n",
    "height, width, channels = env_tr.observation_space.shape\n",
    "\n",
    "print(f'Numero de acciones del juego: {nb_actions}')\n",
    "print(f'Tipo de acciones disponibles en el juego: {env_tr.unwrapped.get_action_meanings()}')\n",
    "print(f'Dimensiones del Frame del juego: [{height},{width},{channels}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "#### 2.3.2 Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de la imagen de entrada: (84, 84, 4)\n",
      "Numero de clases/acciones: 6\n",
      "Model: \"Double DQN_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 20, 20, 16)        4112      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 9, 9, 32)          8224      \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "fc_shared (Dense)            (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "Q_values (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 223,190\n",
      "Trainable params: 223,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definimos la arquitectura de red neuronal en una funcion, asi podemos llamarla directamente y pasarle los parametros que vamos a evaluar.\n",
    "def BuildDcnn(n_classes=6):\n",
    "    model = Sequential(name= 'Double DQN_CNN')\n",
    "    # Entrada\n",
    "    model.add(Permute((2, 3, 1), input_shape=(ef.WINDOW_LENGTH,) + ef.INPUT_SHAPE))\n",
    "\n",
    "    # Base Model - Bloque convolucional\n",
    "    model.add(Conv2D(16, (8, 8), strides=(4, 4), padding='valid', activation=\"relu\", name=\"conv1\"))\n",
    "    model.add(Conv2D(32, (4, 4), strides=(2, 2), padding='valid', activation=\"relu\", name=\"conv2\"))\n",
    "    model.add(Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation=\"relu\", name=\"conv3\"))\n",
    "    \n",
    "    # Top Model - Capa FC.\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "    model.add(Dense(128, activation=\"relu\", name=\"fc_shared\"))\n",
    "    \n",
    "    # Capa final: Q(s,a) \n",
    "    model.add(Dense(n_classes, activation=\"linear\", name=\"Q_values\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# mostramos datos entrada\n",
    "print(\"Tamaño de la imagen de entrada:\", ef.IM_SHAPE)\n",
    "print(\"Numero de clases/acciones:\", nb_actions)\n",
    "\n",
    "# Llamamos a la funcion de construccion de la red del modelo. Esta red neuronal: recibe el estado s, produce vector con los valores Q(s,a), en este caso\n",
    "# tendremos un vector con 6 componentes (6 acciones). La red evalua cual es la mejor Q(s,a).\n",
    "model = BuildDcnn(nb_actions)\n",
    "\n",
    "# Mostramos la arquitectura de la red declarada\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "#### 2.3.3 Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente creado con exito.\n"
     ]
    }
   ],
   "source": [
    "# Definimos nuestro agente: Buffer memoria, policy y tipo agente.\n",
    "def BuildAgent(model, memory_size, tr_steps, n_actions, lr=0.0005):\n",
    "    \n",
    "    # Preparación de la memoria (usamos memory replay en DQN).\n",
    "    ReplayBuffer = SequentialMemory(limit=memory_size, window_length=ef.WINDOW_LENGTH)\n",
    "\n",
    "    # Llamamos a clase para pre-procesado de las observaciones. \n",
    "    processor = AtariProcessor()\n",
    "\n",
    "    # Definimos la policy, estrategia para seleccionar las acciones. Decide que accion tomar.\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),     # Estrategia E-greedy.\n",
    "        attr='eps',\n",
    "        value_max=1.0,          # Explora totalmente al inicio\n",
    "        value_min=0.05,         # Explora poco al final del entrenamiento\n",
    "        value_test=0.05,        # Exploración mínima en test\n",
    "        nb_steps=tr_steps       # Duración del decaimiento\n",
    "    )\n",
    "    \n",
    "    # Definición del agente DQN. Aqui se junta todo, es el cerebro que coordina todo.\n",
    "    # Llama a la ANN para estimar la Q, aplica la policy para elegir la acción, interactua con el entorno y usa el replay bufer para guardar las experiencias,\n",
    "    # entrena la red mediante el target network para estabilizar el aprendizaje y la actualiza con la función de perdidas.\n",
    "    dqn = DQNAgent(\n",
    "        model= model,                    # La red CNN que estima la distribución Q\n",
    "        nb_actions= n_actions,           # Número de acciones posibles en el entorno\n",
    "        memory= ReplayBuffer,            # Replay buffer para almacenar transiciones\n",
    "        policy= policy,                  # Política/Estrategia\n",
    "        processor= processor,            # Preprocesado de observaciones\n",
    "        nb_steps_warmup= 50000,          # Pasos iniciales solo explorando (sin entrenar)\n",
    "        target_model_update= 10000,      # Frecuencia de actualización del target network\n",
    "        train_interval= 4,               # Entrenar cada X pasos de interacción\n",
    "        gamma= 0.99                      # Factor de descuento para recompensas futuras\n",
    "    )\n",
    "\n",
    "    # Compilación del agente\n",
    "    dqn.compile(Adam(learning_rate=lr, clipnorm=10.0), metrics=['mae'])\n",
    "    \n",
    "    # Mostrar confirmacion\n",
    "    print(\"Agente creado con exito.\")\n",
    "    \n",
    "    return dqn\n",
    "\n",
    "# Construimos el agente.\n",
    "agent = BuildAgent(model, Memory_Size, Training_Steps, nb_actions, 0.00025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3.4 Entrenamiento del Agente DQN\n",
    "Configuramos y ejecutamos el entrenamiento del agente. Se realiza un entrenamiento en 3 etapas, con diferente numero de steps e hiperparametros del agente en cada una de ellas. Al final de este a apartado se analizan los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.1 Etapa 1: Entrenamiento desde 0 con 1.5M steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenamos agente.\n",
    "n_tr = 1\n",
    "TrainAgent(agent, env_tr, Training_Steps, N_Solucion, n_train=n_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test de performance del agente\n",
    "n_test= 100\n",
    "TestAgent(agent, env_te, N_Solucion, n_tr, n_test, Episodios_min, Reward_min, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.2 Etapa 2: Fine Tuning de 750k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fijamos steps.\n",
    "n_tr = 2\n",
    "Training_Steps = 750000\n",
    "decay_steps = int(Training_Steps*0.75)\n",
    "\n",
    "# Reducimos learning rate para fine‑tuning\n",
    "LearningRate = 0.000015\n",
    "K.set_value(agent.model.optimizer.learning_rate, LearningRate)\n",
    "\n",
    "# Actualizamos estrategia agente\n",
    "# Continuamos con el epsilon final de la etapa anterior y bajamos a 0.01 con el 75% de los steps.\n",
    "# El 25% final el epsilon sera muy bajo para entrenar practicamente determinista.\n",
    "\n",
    "agent.policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=0.05,      # Continua donde lo habia dejado\n",
    "    value_min=0.01,      # termina casi determinista (1% aleatorio)\n",
    "    value_test=0.0,      # test sin azar\n",
    "    nb_steps=decay_steps\n",
    ")\n",
    "\n",
    "# Aumentamos el numero de pasos de actualizacion de la red para darle mas estabilidad.\n",
    "agent.target_model_update= 20000\n",
    "\n",
    "# Entrenamos agente.\n",
    "TrainAgent(agent, env_tr, Training_Steps, N_Solucion, n_train=n_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test de performance del agente\n",
    "n_test= 100\n",
    "TestAgent(agent, env_te, N_Solucion, n_tr, n_test, Episodios_min, Reward_min, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.3 Etapa 3: Entrenamiento final de 500k steps adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fijamos steps.\n",
    "n_tr = 3\n",
    "Training_Steps = 500000\n",
    "decay_steps = int(Training_Steps*0.80)\n",
    "\n",
    "# Reducimos learning rate para fine‑tuning\n",
    "LearningRate = 0.00001\n",
    "K.set_value(agent.model.optimizer.learning_rate, LearningRate)\n",
    "\n",
    "# Actualizamos estrategia agente\n",
    "# Subimos el epsilon final de la etapa anterior para permitir exploracion de nuevo (10%) y bajamos a 0.005 con el 80% de los steps.\n",
    "# El 20% final el epsilon sera muy bajo para entrenar practicamente determinista.\n",
    "\n",
    "agent.policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=0.10,      # Explora de nuevo un 10% nuevo\n",
    "    value_min=0.005,     # termina casi determinista (0.5% aleatorio)\n",
    "    value_test=0.0,      # test sin azar\n",
    "    nb_steps=decay_steps\n",
    ")\n",
    "\n",
    "# Aumentamos el numero de pasos de actualizacion de la red para darle mas estabilidad.\n",
    "agent.target_model_update= 20000\n",
    "\n",
    "# Entrenamos agente.\n",
    "TrainAgent(agent, env_tr, Training_Steps, N_Solucion, n_train=n_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test de performance del agente\n",
    "n_test= 100\n",
    "TestAgent(agent, env_te, N_Solucion, n_tr, n_test, Episodios_min, Reward_min, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "#### 2.3.5 Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graficamos evolución de las 3 fases del entrenamiento\n",
    "json_paths = [\n",
    "    \"Solucion2/dqn2_SpaceInvaders-v0_log_1.json\",\n",
    "    \"Solucion2/dqn2_SpaceInvaders-v0_log_2.json\",\n",
    "    \"Solucion2/dqn2_SpaceInvaders-v0_log_3.json\",\n",
    "]\n",
    "\n",
    "merge_json_logs(json_paths, \"Solucion2/dqn2_SpaceInvaders-v0_log_FULL.json\")\n",
    "\n",
    "plot_rewards(\"Solucion2/dqn2_SpaceInvaders-v0_log_FULL.json\", 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test de performance del agente durante 300 episodios\n",
    "n_test= 300\n",
    "TestAgent(agent, env_te, N_Solucion, n_tr, n_test, Episodios_min, Reward_min, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha realizado un entrenamiento por etapas:\n",
    "- Etapa1: Inicialmente se optó por entrenar el modelo con 1.5M de steps. A la vista de los resultados mostrados en la curva de entrenamiento, se identificó que el modelo mantenia una pendiente ascendente en el reward medio por lo que todavia tenia margen de mejora. Se ha utilizado un learning rate de 0.00025 y una estrategia E-greedy con decaimiento progresivo desde 1 a 0.05. La red target se actualiza cada 10k steps.\n",
    "- Etapa2: Se optó por una fase de ajuste fino de 750k steps adicionales para maximizar la política de explotación. Se ha utilizado un learning rate de 0.000015 y una estrategia E-greedy con decaimiento progresivo desde 0.05 a 0.01 para el 75% de los steps dejando el 25% final con epsilon fijo para hacer entrenamiento mas determinista. La red target se actualiza cada 20k steps para dar más estabilidad al aprendizaje y ayudar a que el error de los valores Q no oscile tanto.\n",
    "- Etapa3: En vista a los resultados de la etapa anterior, se decide hacer una ultima etapa adicional de 500k steps. Se ha utilizado un learning rate de 0.00001 y una estrategia E-greedy con decaimiento progresivo desde 0.1 a 0.005 para el 80% de los steps dejando el 20% final con epsilon fijo para hacer entrenamiento mas determinista. La red target se actualiza cada 20k steps igual que en la etapa anterior.\n",
    "\n",
    "Resultados:\n",
    "- Tras 1.5M steps de entrenamiento, el test del agente ha dado un resultado de 91/100 episodios por encima de 20 puntos y una media de 27.6 puntos, lo cual no está nada mal. El aprendizaje del agente ha ido mejorando progresivamente a lo largo de los 1.5M steps.\n",
    "- Tras hacer un fine tunning de 750K steps adicionales el test del agente ha dado un resultado de 95/100 episodios por encima de 20 puntos y la media ha sido de 33.2 puntos. El agente ha mejorado claramente en esta 2ª etapa.\n",
    "- Finalmente tras 500k steps adicionales el test del agente ha dado un resultado de 100/100 episodios por encima de 20 puntos y la media ha sido de 36.3 puntos. El agente ha mejorado claramente en esta 3ª etapa final hasta superar el reto.\n",
    "- Ejecutamos un test adicional de 300 episodios para ver comportamiento mas amplio. El resultado es de 300/300 episodios por encima de 20 puntos y la media ha sido de 34.17 puntos.\n",
    "- Analizando la curva completa de entrenamiento de 2.75M de steps, vemos que el agente ha tenido un progreso claro a través de las 3 etapas desarrolladas. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GymClasic (Python 3.8)",
   "language": "python",
   "name": "gymclasic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
